# ğŸ§ª Taller - Red Neuronal Convolucional BÃ¡sica en MNIST con PyTorch

## ğŸ—“ï¸ Fecha

2025-06-19

---

## ğŸ¯ Objetivo del Taller

Implementar y entrenar una red neuronal convolucional (CNN) bÃ¡sica para clasificaciÃ³n de dÃ­gitos manuscritos usando el dataset MNIST. Analizar el impacto de los filtros y capas en el desempeÃ±o, y visualizar los resultados obtenidos.

---

## ğŸ§  Conceptos Aprendidos

* Estructura y componentes de una CNN: capas convolucionales, pooling, fully connected, dropout.
* Preprocesamiento y normalizaciÃ³n de imÃ¡genes.
* Entrenamiento supervisado y evaluaciÃ³n de modelos.
* VisualizaciÃ³n de mÃ©tricas: curvas de pÃ©rdida, precisiÃ³n y matriz de confusiÃ³n.
* InterpretaciÃ³n de predicciones y errores del modelo.

---

## ğŸ› ï¸ Herramientas y Entornos

* Python 3.x
* PyTorch y torchvision
* Matplotlib
* Scikit-learn

---

## ğŸ“ Estructura del Proyecto

```
2025-06-19_taller_cnn_basico_deep_learning_keras_pytorch/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ MNIST/
â”œâ”€â”€ modelo/
â”‚   â””â”€â”€ cnn_mnist.pt
â”œâ”€â”€ resultados/
â”‚   â”œâ”€â”€ sample_predictions.png
â”‚   â”œâ”€â”€ accuracy_curve.png
â”‚   â”œâ”€â”€ loss_curve.png
â”‚   â””â”€â”€ confusion_matrix.png
â””â”€â”€ pytorch/
    â””â”€â”€ cnn_pytorch.py
```

---

## ğŸ’¡ ImplementaciÃ³n Destacada

### ğŸ”¹ Carga y preprocesamiento de datos

```python
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])
train_dataset = datasets.MNIST(root=DATA_DIR, train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root=DATA_DIR, train=False, download=True, transform=transform)
```
*Las imÃ¡genes se normalizan para mejorar la estabilidad del entrenamiento.*

### ğŸ”¹ Arquitectura de la CNN

```python
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.fc1 = nn.Linear(32 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)
        self.dropout = nn.Dropout(0.25)
    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 32 * 7 * 7)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x
```
*La red consta de dos capas convolucionales (con filtros 3x3), seguidas de pooling, y dos capas totalmente conectadas. El dropout ayuda a evitar el sobreajuste.*

### ğŸ”¹ Entrenamiento y evaluaciÃ³n

```python
def train():
    model.train()
    running_loss = 0.0
    correct, total = 0, 0
    for images, labels in train_loader:
        images, labels = images.to(DEVICE), labels.to(DEVICE)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * images.size(0)
        _, predicted = torch.max(outputs, 1)
        correct += (predicted == labels).sum().item()
        total += labels.size(0)
    return running_loss / total, correct / total
```
*FunciÃ³n de entrenamiento que actualiza los pesos y calcula la precisiÃ³n.*

### ğŸ”¹ VisualizaciÃ³n de resultados

```python
plt.plot(train_losses, label='Train Loss')
plt.plot(test_losses, label='Test Loss')
plt.xlabel('Ã‰poca')
plt.ylabel('PÃ©rdida')
plt.legend()
plt.title('Curva de pÃ©rdida')
plt.savefig(os.path.join(RESULTS_DIR, 'loss_curve.png'))
```
*Se grafican las curvas de pÃ©rdida para analizar el aprendizaje.*

### ğŸ”¹ Matriz de confusiÃ³n y predicciones

```python
cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(range(10)))
disp.plot(ax=ax)
plt.title('Matriz de confusiÃ³n - MNIST')
plt.savefig(os.path.join(RESULTS_DIR, 'confusion_matrix.png'))
```
*La matriz de confusiÃ³n permite identificar los errores mÃ¡s comunes del modelo.*

---

## ğŸ“Š Resultados Visuales

* Ejemplos de predicciÃ³n:  
  ![sample_predictions.png](resultados/sample_predictions.png)
* Curva de precisiÃ³n durante el entrenamiento:  
  ![accuracy_curve.png](resultados/accuracy_curve.png)
* Curva de pÃ©rdida durante el entrenamiento:  
  ![loss_curve.png](resultados/loss_curve.png)
* Matriz de confusiÃ³n:  
  ![confusion_matrix.png](resultados/confusion_matrix.png)

---

## ğŸ” Prompts Utilizados

* "cÃ³mo crear una CNN simple en PyTorch"
* "cÃ³mo visualizar la matriz de confusiÃ³n en sklearn"
* "cÃ³mo graficar curvas de pÃ©rdida y precisiÃ³n en matplotlib"
* "cÃ³mo guardar y cargar modelos en PyTorch"

---

## ğŸ’¬ ReflexiÃ³n Final

AprendÃ­ que los filtros en las capas convolucionales permiten extraer caracterÃ­sticas locales importantes, como bordes y formas, que son esenciales para el reconocimiento de dÃ­gitos. El uso de pooling reduce la dimensionalidad y ayuda a generalizar mejor. La adiciÃ³n de dropout y el ajuste de la cantidad de filtros/capas mejoraron la precisiÃ³n y redujeron el sobreajuste. Visualizar las curvas y la matriz de confusiÃ³n fue clave para entender el comportamiento del modelo y detectar posibles mejoras. 