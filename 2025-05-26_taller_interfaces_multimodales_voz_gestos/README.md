# ğŸ§ª InteracciÃ³n Multimodal: Gestos y Voz

## ğŸ“… Fecha
`2025-05-26` â€“ Fecha de entrega

---

## ğŸ¯ Objetivo del Taller

Fusionar gestos y comandos de voz para realizar acciones compuestas dentro de una interfaz visual. Este taller introduce los fundamentos de los sistemas de interacciÃ³n multimodal, combinando dos formas de entrada humana para enriquecer la experiencia de control.

---

## ğŸ§  Conceptos Aprendidos

- DetecciÃ³n de movimiento con OpenCV
- Reconocimiento de voz con SpeechRecognition
- Uso de hilos (threading) para entradas simultÃ¡neas
- LÃ³gica condicional multimodal
- VisualizaciÃ³n interactiva con pygame
- RetroalimentaciÃ³n auditiva con pyttsx3

---

## ğŸ”§ Herramientas y Entornos

- Python 3.13
- LibrerÃ­as: opencv-python, pygame, speech_recognition, pyttsx3, numpy, pyaudio

---

## ğŸ“ Estructura del Proyecto

```
2025-05-26_taller_interfaces_multimodales_voz_gestos/
â”œâ”€â”€ python/
â”œâ”€â”€ resultado/
â”œâ”€â”€ README.md
```

---

## ğŸ§ª ImplementaciÃ³n

### ğŸ”¹ Etapas realizadas
1. ConfiguraciÃ³n de entorno Python y librerÃ­as
2. Captura de video desde webcam
3. DetecciÃ³n de gestos a partir del movimiento
5. Captura y reconocimiento de comandos de voz
6. LÃ³gica combinada gesto + voz para controlar una figura
7. RetroalimentaciÃ³n visual (texto en pantalla) y auditiva (voz)

### ğŸ”¹ CÃ³digo relevante

```python
if comando and gesto:
    if gesto == 'mano_abierta':
        if comando in ['rojo', 'azul', 'verde', 'amarillo']:
            color = colores[comando]
            hablar(f"Color cambiado a {comando}")
    elif gesto == 'puno':
        if comando == 'rotar':
            angulo = (angulo + 30) % 360
            hablar("Rotando 30 grados")
    elif gesto == 'v':
        if comando == 'ocultar':
            cuadro_visible = False
            hablar("Ocultando figura")
        elif comando == 'mostrar':
            cuadro_visible = True
            hablar("Mostrando figura")
```

---

## ğŸ“Š Resultados Visuales

El sistema reacciona en tiempo real al movimiento de la cÃ¡mara y un cuadrado dibujado en pantalla responde a las acciones:
- Cambio de color: si se dice el color y se muestra la mano abierta.
- Rotar: si se dice "rotar" y se muestra el puÃ±o cerrado.
- Mostrar u ocultar: diciendo el comando mientras se hace una V con los dedos de una mano.

![resultado.gif](resultados/demo_camara_y_voz.gif)

---

## ğŸ§© Prompts Usados

```text
â€œProvee el cÃ³digo para detectar la siguiente combinaciÃ³n comando-gesto de mano: [...]â€
â€œDame el cÃ³digo para mostrar en terminal el gesto y comando detectado, junto a "escuchando..."
â€œLa detecciÃ³n de cÃ¡mara va muy rÃ¡pido para procesar al tiempo el comando de voz, Â¿cÃ³mo corregirlo?"
```

---

## ğŸ’¬ ReflexiÃ³n Final

Este taller me permitiÃ³ aplicar entradas multimodales y coordinar mÃºltiples hilos de entrada en un solo sistema interactivo. Fue muy Ãºtil para entender cÃ³mo combinar detecciÃ³n visual con audio, y cÃ³mo simplificar gestos sin dependencias complejas como MediaPipe.

La parte mÃ¡s desafiante fue sincronizar el tiempo de reconocimiento de voz con los gestos. SolucionÃ© esto aÃ±adiendo una pausa mÃ­nima entre gestos detectados. Para futuros proyectos.

---

## âœ… Checklist de Entrega

- [x] Carpeta `2025-05-26_taller_interfaces_multimodales_voz_gestos`
- [x] CÃ³digo limpio y funcional
- [x] VisualizaciÃ³n implementada en pygame
- [x] RetroalimentaciÃ³n auditiva con voz sintetizada
- [x] README completo y claro
